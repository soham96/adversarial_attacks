# Adversarial Attacks

This repository has code for addding adversarial noise to images to change the predicted class of an image without changing what the image looks like. Below is an example of changing the class of an image of a Giant Panda that was correctly predicted by a resnet50 model to Gorilla!

![Screenshot 2024-06-12 at 11 50 29 PM](https://github.com/soham96/adversarial_attacks/assets/18757535/0997528c-a022-44e2-88c9-a0af31b7741c)

You can also try out the notebook in Google Colab. Here is the [link](https://colab.research.google.com/drive/1dhmRRvoonvfgpxgdhTIQ9MkrpAIDYdY1?usp=sharing)
